{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Waymo Open Dataset 3D Semantic Segmentation Tutorial.ipynb",
      "provenance": [
        {
          "file_id": "tutorial_3d_semseg.ipynb",
          "timestamp": 1649874845881
        },
        {
          "file_id": "tutorial.ipynb",
          "timestamp": 1644287712198
        }
      ],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Waymo Open Dataset 3D Semantic Segmentation Tutorial\n",
        "\n",
        "- Website: https://waymo.com/open\n",
        "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
        "\n",
        "This tutorial demonstrates how to decode and interpret the 3D semantic segmentation labels. Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
        "\n",
        "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
        "\n",
        "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\".\n",
        "\n"
      ],
      "metadata": {
        "id": "-pVhOfzLx9us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Installation"
      ],
      "metadata": {
        "id": "1sPLur9kMaLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package installation\n",
        "Please follow the instructions in [tutorial.ipynb](https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial.ipynb)."
      ],
      "metadata": {
        "id": "iEsf_G5_MeS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and global definitions"
      ],
      "metadata": {
        "id": "rqs8_62VNc4T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip3 install tfrecord"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch\n",
        "import tfrecord\n",
        "\n",
        "EXPORT_LOCATION = '/export/tf_output.tfrecord'\n",
        "DATA = None\n",
        "\n",
        "def generateTFrecord(data: torch.Tensor):\n",
        "    writer = tfrecord.TFRecordWriter(\"/export/tf_output.tfrecord\")\n",
        "    for index in range(len(data)):\n",
        "        frame, range_images, segmentation_labels, ri_index = data[index]\n",
        "        writer.write({\n",
        "            \"frame\": (frame, \"byte\"),\n",
        "            \"range_images\": (range_images, \"Dict\"),\n",
        "            \"segmentation_labels\": (segmentation_labels, \"Dict\"),\n",
        "            \"index\": (ri_index, \"int\")\n",
        "        })\n",
        "    writer.close()\n",
        "\n",
        "generateTFrecord(DATA)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Data location. Please edit.\n",
        "\n",
        "# A tfrecord containing tf.Example protos as downloaded from the Waymo dataset\n",
        "# webpage.\n",
        "\n",
        "# Replace this path with your own tfrecords.\n",
        "FILENAME = '/export/tf_output.tfrecord'"
      ],
      "outputs": [],
      "metadata": {
        "id": "YuNAlbQpNkLa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from waymo_open_dataset.utils import  frame_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
        "from waymo_open_dataset.protos import segmentation_metrics_pb2\n",
        "from waymo_open_dataset.protos import segmentation_submission_pb2"
      ],
      "outputs": [],
      "metadata": {
        "id": "xCDNLdp9Ni8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read 3D semantic segmentation labels from Frame proto\n",
        " Note that only a subset of the frames have 3d semseg labels."
      ],
      "metadata": {
        "id": "ibor0U9XBlX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
        "for data in dataset:\n",
        "    frame = open_dataset.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    if frame.lasers[0].ri_return1.segmentation_label_compressed:\n",
        "      break"
      ],
      "outputs": [],
      "metadata": {
        "id": "O41R3lljM9Ym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(frame.context.name)\n",
        "print(frame.context.stats)"
      ],
      "outputs": [],
      "metadata": {
        "id": "opFz4B9JXC7p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "(range_images, camera_projections, segmentation_labels,\n",
        " range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(\n",
        "    frame)"
      ],
      "outputs": [],
      "metadata": {
        "id": "wHK95_JBUXUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(segmentation_labels[open_dataset.LaserName.TOP][0].shape.dims)"
      ],
      "outputs": [],
      "metadata": {
        "id": "fgCDPt9zeV_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import zlib\n",
        "\n",
        "def compress_array(array: np.ndarray, is_int32: bool = False):\n",
        "  \"\"\"Compress a numpy array to ZLIP compressed serialized MatrixFloat/Int32.\n",
        "\n",
        "  Args:\n",
        "    array: A numpy array.\n",
        "    is_int32: If true, use MatrixInt32, otherwise use MatrixFloat.\n",
        "\n",
        "  Returns:\n",
        "    The compressed bytes.\n",
        "  \"\"\"\n",
        "  if is_int32:\n",
        "    m = open_dataset.MatrixInt32()\n",
        "  else:\n",
        "    m = open_dataset.MatrixFloat()\n",
        "  m.shape.dims.extend(list(array.shape))\n",
        "  m.data.extend(array.reshape([-1]).tolist())\n",
        "  return zlib.compress(m.SerializeToString())\n",
        "\n",
        "def decompress_array(array_compressed: bytes, is_int32: bool = False):\n",
        "  \"\"\"Decompress bytes (of serialized MatrixFloat/Int32) to a numpy array.\n",
        "\n",
        "  Args:\n",
        "    array_compressed: bytes.\n",
        "    is_int32: If true, use MatrixInt32, otherwise use MatrixFloat.\n",
        "\n",
        "  Returns:\n",
        "    The decompressed numpy array.\n",
        "  \"\"\"\n",
        "  decompressed = zlib.decompress(array_compressed)\n",
        "  if is_int32:\n",
        "    m = open_dataset.MatrixInt32()\n",
        "    dtype = np.int32\n",
        "  else:\n",
        "    m = open_dataset.MatrixFloat()\n",
        "    dtype = np.float32\n",
        "  m.ParseFromString(decompressed)\n",
        "  return np.array(m.data, dtype=dtype).reshape(m.shape.dims)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "TOP_LIDAR_ROW_NUM = 64\n",
        "TOP_LIDAR_COL_NUM = 2650"
      ],
      "outputs": [],
      "metadata": {
        "id": "bZTjFN_CNlNx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_range_image_point_indexing(range_images, ri_index=0):\n",
        "  \"\"\"Get the indices of the valid points (of the TOP lidar) in the range image.\n",
        "\n",
        "  The order of the points match those from convert_range_image_to_point_cloud\n",
        "  and convert_range_image_to_point_cloud_labels.\n",
        "\n",
        "  Args:\n",
        "    range_images: A dict of {laser_name, [range_image_first_return,\n",
        "       range_image_second_return]}.\n",
        "    ri_index: 0 for the first return, 1 for the second return.\n",
        "\n",
        "  Returns:\n",
        "    points_indexing_top: (N, 2) col and row indices of the points in the\n",
        "      TOP lidar.\n",
        "  \"\"\"\n",
        "  points_indexing_top = None\n",
        "  xgrid, ygrid = np.meshgrid(range(TOP_LIDAR_COL_NUM), range(TOP_LIDAR_ROW_NUM))\n",
        "  col_row_inds_top = np.stack([xgrid, ygrid], axis=-1)\n",
        "  range_image = range_images[open_dataset.LaserName.TOP][ri_index]\n",
        "  range_image_tensor = tf.reshape(\n",
        "      tf.convert_to_tensor(range_image.data), range_image.shape.dims)\n",
        "  range_image_mask = range_image_tensor[..., 0] > 0\n",
        "  points_indexing_top = col_row_inds_top[np.where(range_image_mask)]\n",
        "  return points_indexing_top"
      ],
      "outputs": [],
      "metadata": {
        "id": "5P9hToEvNUUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create Pred (Val)\n",
        "\n",
        "# Replace this path with the real path to the WOD validation set folder.\n",
        "folder_name = '/content/waymo-od/.../validation/'\n",
        "\n",
        "filenames = [os.path.join(folder_name, x) for x in os.listdir(\n",
        "    folder_name) if 'tfrecord' in x]\n",
        "assert(len(filenames) == 202)\n",
        "\n",
        "segmentation_frame_list = segmentation_metrics_pb2.SegmentationFrameList()\n",
        "for idx, filename in enumerate(filenames):\n",
        "  if idx % 10 == 0:\n",
        "    print('Processing %d/%d run segments...' % (idx, len(filenames)))\n",
        "  dataset = tf.data.TFRecordDataset(filename, compression_type='')\n",
        "  for data in dataset:\n",
        "    frame = open_dataset.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    if frame.lasers[0].ri_return1.segmentation_label_compressed:\n",
        "      segmentation_frame = dummy_semseg_for_one_frame(frame)  # TODO: Replace with our semseg\n",
        "      segmentation_frame_list.frames.append(segmentation_frame)\n",
        "print('Total number of frames: ', len(segmentation_frame_list.frames))\n",
        "\n",
        "# Create Pred (Test)\n",
        "# Create the dummy pred file for the testing set run segments.\n",
        "\n",
        "# Replace the paths with the real paths to the WOD testing set folders.\n",
        "folder_name1 = '/content/waymo-od/.../testing/'\n",
        "folder_name2 = '/content/waymo-od/.../testing_location/'\n",
        "filenames1 = [os.path.join(folder_name1, x) for x in os.listdir(\n",
        "    folder_name1) if 'tfrecord' in x]\n",
        "filenames2 = [os.path.join(folder_name2, x) for x in os.listdir(\n",
        "    folder_name2) if 'tfrecord' in x]\n",
        "filenames = filenames1 + filenames2\n",
        "print(len(filenames))\n",
        "assert(len(filenames) == 150)\n",
        "\n",
        "# Replace this path with the real path. The file is under:\n",
        "# /waymo-open-dataset/tutorial/ in the github repo.\n",
        "# Each line of the file is the \"<context_name>, <timestamp_micros>\" of a frame\n",
        "# with semseg labels. \n",
        "testing_set_frame_file = '/path/3d_semseg_test_set_frames.txt'\n",
        "context_name_timestamp_tuples = [x.rstrip().split(',') for x in (\n",
        "    open(testing_set_frame_file, 'r').readlines())]\n",
        "\n",
        "segmentation_frame_list = segmentation_metrics_pb2.SegmentationFrameList()\n",
        "for idx, filename in enumerate(filenames):\n",
        "  if idx % 10 == 0:\n",
        "    print('Processing %d/%d run segments...' % (idx, len(filenames)))\n",
        "  dataset = tf.data.TFRecordDataset(filename, compression_type='')\n",
        "  for data in dataset:\n",
        "    frame = open_dataset.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    context_name = frame.context.name\n",
        "    timestamp = frame.timestamp_micros\n",
        "    if (context_name, str(timestamp)) in context_name_timestamp_tuples:\n",
        "      print(context_name, timestamp)\n",
        "      segmentation_frame = dummy_semseg_for_one_frame(frame)  # TODO: Replace with our semseg\n",
        "      segmentation_frame_list.frames.append(segmentation_frame)\n",
        "print('Total number of frames: ', len(segmentation_frame_list.frames))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a submission file for the validation set"
      ],
      "metadata": {
        "id": "37g8RIQhNMm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create the submission file, which can be uploaded to the eval server.\n",
        "submission = segmentation_submission_pb2.SemanticSegmentationSubmission()\n",
        "submission.account_name = 'zxiang@stanford.edu'  # Idk whos email you wanna use\n",
        "submission.unique_method_name = 'InterNet'  # Get it? :P (TODO: change name)\n",
        "submission.affiliation = 'Stanford University'\n",
        "submission.authors.append('Albin Mosskull')\n",
        "submission.authors.append('Yuntao (Tommy) Ma')\n",
        "submission.authors.append('Zhengbo (Alana) Xiang')\n",
        "submission.description = \"A SqueezeNet-inspired 3D semantic segmantation method (val set).\"\n",
        "submission.method_link = 'NA'\n",
        "submission.sensor_type = 1\n",
        "submission.number_past_frames_exclude_current = 2\n",
        "submission.number_future_frames_exclude_current = 0\n",
        "submission.inference_results.CopyFrom(segmentation_frame_list)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0_YlqK4RR8pR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "output_filename = '/tmp/wod_semseg_val_set_pred_submission.bin'\n",
        "f = open(output_filename, 'wb')\n",
        "f.write(submission.SerializeToString())\n",
        "f.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "hrCD59OkR_wK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a submission file for the testing set"
      ],
      "metadata": {
        "id": "GIjNwn3bSTec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create the submission file, which can be uploaded to the eval server.\n",
        "submission = segmentation_submission_pb2.SemanticSegmentationSubmission()\n",
        "submission.account_name = 'zxiang@stanford.edu'\n",
        "submission.unique_method_name = 'InterNet'\n",
        "submission.affiliation = 'Stanford University'\n",
        "submission.authors.append('Albin Mosskull')\n",
        "submission.authors.append('Yuntao (Tommy) Ma')\n",
        "submission.authors.append('Zhengbo (Alana) Xiang')\n",
        "submission.description = \"A SqueezeNet-inspired 3D semantic segmantation method (test set).\"\n",
        "submission.method_link = 'NA'\n",
        "submission.sensor_type = 1\n",
        "submission.number_past_frames_exclude_current = 2\n",
        "submission.number_future_frames_exclude_current = 0\n",
        "submission.inference_results.CopyFrom(segmentation_frame_list)"
      ],
      "outputs": [],
      "metadata": {
        "id": "30q-hzjQljYc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "output_filename = '/tmp/wod_semseg_test_set_pred_submission.bin'\n",
        "f = open(output_filename, 'wb')\n",
        "f.write(submission.SerializeToString())\n",
        "f.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "oStSwaE-Sv5Y"
      }
    }
  ]
}