# Task
# Available Task: [Segmentation3DTask]
task: 'Segmentation3DTask'

# General Setting - log path
exp_name: "experiment name"
save_dir: "PATH/TO/RESULT/DIR/"
version: 0

# General Setting - GPU
gpus: 10
strategy: "ddp"

# General setting - data parameter
num_cls: 23
loader_worker: 16
train_path: '/scr/data/h5/train.h5'
val_path: '/scr/data/h5/validate.h5'
test_path: '/scr/data/h5/test.h5'
# Full set of keys you can use
# ['range_image', 'image', 'ri1_range_image', 'ri2_range_image', 'ri1_label', 'ri2_label', 'ri1_proj', 'ri2_proj', 'proj_pixel']
key_to_load: ['range_image', 'ri1_label']

# General setting - model
# available encoder:
# Salsa: {num_inchannel}
encoder: Salsa
encoder_parm: {num_inchannel: 6}
# available decoder:
# Salsa: {}
decoder: Salsa
decoder_parm: {}
# available head:
# Salsa: {num_features, num_cls}
head: Salsa
head_parm: {num_features: 32, num_cls: 23}

# loss
# Add Lovasz Softmax loss to Cross Entropy Loss
lavasz_softmax: True
# Use 1st order polynomial coefficient perturbation with eps=1
poly1_Xentropy: True
# CE + poly1_eps * (1-Pt)
poly1_eps: 1
# Use inverse class distribution to weight the 
# class frequency in /docs/
weighted_Xentropy: True
# weight = 1 / (class_frequency + class_weight_eps)
class_weight_eps: 0.001


# General setting - training parameter
batch_size: 20
max_epochs: 2

# lr warmup/decay
# Maximum learning rate warmup to linearly
# this must be greater than learning_rate
learning_rate: 0
peak_learning_rate: 0.001
# The final learning rate the optimizer decay to after reaching peak_learning_rate
# this should be smaller than peak_learning_rate
final_learning_rate: 0.000001
# Number of batch required to reach peak_learning_rate from start
lr_warmup_batch: 500
# Number of batch required to reach final_learning_rate after reach peak_learning_rate
lr_decay_batch: 24000
# For constant learning rate, you can set learning_rate = peak_learning_rate = final_learning_rate

# General setting - lightning specific 
gradient_clip_val: 0.5
enable_model_summary: False
limit_train_batches: 1.0
# Metric to monitor for early stopping and ckpt saving
# The following metric are always available:
# ["Val/epoch_avg_loss", "Val/epoch_avg_mIoU", "Train/epoch_avg_loss", "Train/epoch_avg_mIoU"]
# Of course, there are some more metrics you can monitor or add your own to the train/eval step. 
monitor_metric: "Val/epoch_avg_mIoU"
# The direction of above metric needs to go, i.e. min means you wish the metric go down
# Choices: "min" or "max"
monitor_mode: "max"
# Only save top k epoch's ckpt for above metrics. 
# Set to -1 if you wish to save every epoch. 
save_top_k: 5
# How many epoch the above metric did not change significantly to trigger the early stopping
# You can set this equal to max_epochs to train for max_epochs 
patience: 10